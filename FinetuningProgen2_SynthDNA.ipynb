{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hDxfeTstU4e"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers\n",
        "!pip install accelerate\n",
        "!pip install -U datasets\n",
        "!pip install scikit-learn\n",
        "!pip install peft\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Move your tensors and model to the GPU\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU is not available, using CPU\")"
      ],
      "metadata": {
        "id": "gaJlvnchtaBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc45b45-2859-4914-c486-dc3bcea30510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from tokenizers import Tokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tokenizers.processors import BertProcessing\n",
        "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# load model and tokenizer\n",
        "modelOne = AutoModelForCausalLM.from_pretrained(\"hugohrban/progen2-base\", trust_remote_code=True,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type='nf4'\n",
        "        ))\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1, target_modules=['qkj_proj', 'out_proj', 'fc_in', 'fc_out']\n",
        ")\n",
        "model = get_peft_model(modelOne, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "tokenizer = Tokenizer.from_file('/workspace/tokenizer.json')\n",
        "#tokenizer = Tokenizer.from_file('tokenizer.json')"
      ],
      "metadata": {
        "id": "d9t-ApluteDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "5ZZ72ZJUPli6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/workspace/SynthDNA Polymerase Dataset Update.csv\")\n",
        "#df = pd.read_csv(\"SynthDNA Polymerase Dataset Update.csv\")\n",
        "sequences = df.iloc[:, 0].tolist()"
      ],
      "metadata": {
        "id": "ZYpRsmRGooRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from tokenizers import Tokenizer\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import subprocess\n",
        "from itertools import combinations\n",
        "\n",
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, sequences, tokenizer, begin_token_id, end_token_id):\n",
        "        self.tokenized_sequences = [tokenizer.encode(sequence, add_special_tokens=False) for sequence in sequences]\n",
        "        for i, encoding in enumerate(self.tokenized_sequences):\n",
        "            modified_ids = encoding.ids\n",
        "            self.tokenized_sequences[i] = modified_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.tokenized_sequences[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    max_length = max(len(sequence) for sequence in batch)\n",
        "    padded_input_ids = torch.tensor(\n",
        "        [sequence + [0] * (max_length - len(sequence)) for sequence in batch], dtype=torch.long)\n",
        "    return {\"input_ids\": padded_input_ids, \"labels\": padded_input_ids.clone()}"
      ],
      "metadata": {
        "id": "T0sZBq8OMwEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences, test_sequences = train_test_split(sequences, test_size=0.1, random_state=42)\n",
        "test_dataset = ProteinDataset(test_sequences, tokenizer, begin_token_id=1, end_token_id=2)\n",
        "train_dataset = ProteinDataset(train_sequences, tokenizer, begin_token_id=1, end_token_id=2)\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "metadata": {
        "id": "TdTVRndIYBua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e84e49f2-d119-4fe6-92b1-23a92fd246c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50182\n",
            "5576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/workspace/results',\n",
        "    eval_strategy=\"epoch\",    # output directory          # evaluate at the end of each epoch\n",
        "    num_train_epochs=10,                  # total number of training epochs\n",
        "    per_device_train_batch_size=2,       # batch size per device during training\n",
        "    per_device_eval_batch_size=2,        # batch size for evaluation\n",
        "    warmup_steps=500,# reduced number of warmup steps for learning rate scheduler\n",
        "    gradient_accumulation_steps=4,\n",
        "    weight_decay=0.01,                    # strength of weight decay\n",
        "    logging_dir='/workspace/logs',\n",
        "    logging_steps=10,                     # directory for storing logs\n",
        "    learning_rate=1e-5,                   # learning rate\n",
        "    save_total_limit=1,                   # limit the total number of checkpoints                       # save checkpoints every 5000 steps\n",
        "    auto_find_batch_size=False,            # enable auto batch size adjustment\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=lambda data: collate_fn(data)\n",
        ")"
      ],
      "metadata": {
        "id": "jbW0dpXdtoSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "7Uo_xCIwRTEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('/workspace/finetuned_progen2')\n",
        "tokenizer.save_pretrained('/workspace/finetuned_progen2')"
      ],
      "metadata": {
        "id": "TBFKoy84Ful0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_finetuned = AutoModelForCausalLM.from_pretrained('/workspace/results/checkpoint-1220', trust_remote_code=True, torch_dtype='auto')"
      ],
      "metadata": {
        "id": "SIO2_-ivRkqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "gpR3RsYIFqFz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}