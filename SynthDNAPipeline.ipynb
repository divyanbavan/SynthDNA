{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SynthDNA Pipeline**\n",
        "\n",
        "This notebook contains the code used to generate and filter DNA polymerase sequences I tested. There are three main sections: sequence generation, filtering, and fidelity scoring."
      ],
      "metadata": {
        "id": "aMNlx4XHfNW0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDl1LjyFoazp"
      },
      "outputs": [],
      "source": [
        "# Installing dependencies\n",
        "\n",
        "!pip install -U transformers\n",
        "!pip install accelerate\n",
        "!pip install -U datasets\n",
        "!pip install scikit-learn\n",
        "!pip install peft\n",
        "!pip install -U bitsandbytes\n",
        "!apt-get update\n",
        "!apt-get install -y clustalo\n",
        "!pip install biopython\n",
        "!pip install deepspeed\n",
        "!apt-get -qq install cd-hit\n",
        "!pip install bioemu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1: Generating Sequences**"
      ],
      "metadata": {
        "id": "VWB24rhmf0DQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCiC4BmZofSR",
        "outputId": "a760b130-33fb-4891-f9c5-26dbdecb5486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "# Setting device to GPU\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU is not available, using CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUKdplbkoh5A"
      },
      "outputs": [],
      "source": [
        "# loading model for sequence generation\n",
        "\n",
        "# make sure to add tokenizer file to runtime\n",
        "\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from tokenizers import Tokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch.nn.functional as F\n",
        "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
        "import deepspeed\n",
        "\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\"divyanb/SynthDNA-Model\", trust_remote_code=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        quantization_config=BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type='nf4'\n",
        "        ))\n",
        "\n",
        "tokenizer = Tokenizer.from_file('tokenizer.json')\n",
        "\n",
        "ds_inference_config = {\n",
        "    \"replace_with_kernel_inject\": True,\n",
        "    \"dtype\": torch.float16,\n",
        "}\n",
        "\n",
        "model = deepspeed.init_inference(model, config=ds_inference_config, mp_size=1)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvy6o0ysok3X"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "num_sequences = 5000\n",
        "generated_sequences = []\n",
        "\n",
        "tokenized_sequence = tokenizer.encode('2SKKINLWSTLGVQRTKQYRLDEKRYGFGELIRLVAPLVQNEIYYEADYKHKKPDYEEALIARNSIPGDGRLVIYGIVMGPKIKVGKAALKKAVAVHPGIAKYEHLPRTIQEYIALKEPPIEYNALKQIVEKVIRVAEEVDGHKLITELVRAQTEKAIESWDRRVIELGRTIVKGEEDIVAYRKKTVFFGRKYFGEYELELLGPLKSNIYKVFELAKKKIEESEGGPITAYLGDTDIYLVKFGFKEELEKWVLEIYKRGWATVSEACEKCYWRAKAYGYYGYFSNALLKIAKQRYDLLIKEIPDQTEKMKTKIKQREELLHGLLSPIFGPIDKCFKHGVQPAIDYNKCGELNLTDPSVNHTIIISPYLARFDLYVINEWLGKEPEKVFGGTYSERLRRQYEEESPKNPAVENREYAKRLLFWEVLN')\n",
        "\n",
        "input_tensor = torch.tensor([tokenized_sequence.ids]).to(device)\n",
        "\n",
        "for i in range(0, num_sequences, batch_size):\n",
        "    current_batch_size = min(batch_size, num_sequences - i)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_tensor,\n",
        "            max_length=800,\n",
        "            pad_token_id=tokenizer.encode('<|pad|>').ids[0],\n",
        "            do_sample=True,\n",
        "            top_p=0.5, #for N term constant I used 0.9\n",
        "            temperature=0.5, #for N term constant I used 1.0\n",
        "            num_return_sequences=current_batch_size\n",
        "        )\n",
        "\n",
        "    as_lists = lambda batch: [batch[i, ...].detach().cpu().numpy().tolist() for i in range(batch.shape[0])]\n",
        "    sequences = tokenizer.decode_batch(as_lists(output))\n",
        "\n",
        "    if len(sequences) > 0:\n",
        "        sequences = [x.replace('2', '') for x in sequences]\n",
        "        sequences = [x.replace('1', '') for x in sequences]\n",
        "\n",
        "    generated_sequences.extend(sequences)\n",
        "    print(len(generated_sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMTzPQSdKtC2"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open('batchX.csv', mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    for string in generated_sequences:\n",
        "        writer.writerow([string])\n",
        "\n",
        "print(\"CSV file created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2: Filtering Sequences**"
      ],
      "metadata": {
        "id": "5xNm7AK7hz0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read CSV file with generated sequences\n",
        "\n",
        "import csv\n",
        "\n",
        "with open('batchX.csv', 'r', newline='') as file:\n",
        "    reader = csv.reader(file)\n",
        "    sequences = [row[0] for row in reader]\n",
        "\n",
        "print(len(sequences))"
      ],
      "metadata": {
        "id": "SnsuzD5_1JRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eK7ne_GRonjz"
      },
      "outputs": [],
      "source": [
        "# Define functions for filtering\n",
        "\n",
        "import subprocess\n",
        "import re\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "# Function to run Clustal Omega for two sequences\n",
        "def run_clustal_omega(seq1, seq2, output_file):\n",
        "    with open(\"input_sequences.fasta\", \"w\") as f:\n",
        "        f.write(f\">Sequence1\\n{seq1}\\n\")\n",
        "        f.write(f\">Sequence2\\n{seq2}\\n\")\n",
        "\n",
        "    clustal_cmd = [\n",
        "        \"clustalo\",\n",
        "        \"-i\", \"input_sequences.fasta\",\n",
        "        \"-o\", output_file,\n",
        "        \"--force\",\n",
        "        \"--outfmt\", \"clu\"\n",
        "    ]\n",
        "\n",
        "    subprocess.run(clustal_cmd)\n",
        "\n",
        "    with open(output_file, \"r\") as aligned_file:\n",
        "        alignment = aligned_file.read()\n",
        "    return alignment\n",
        "\n",
        "# Function to parse the Clustal Omega alignment file\n",
        "def parse_alignment_file(alignment_file):\n",
        "    sequences = {}\n",
        "    with open(alignment_file, 'r') as file:\n",
        "        current_sequence = \"\"\n",
        "        for line in file:\n",
        "            # Skip headers or other non-sequence lines\n",
        "            if line.startswith(\" \") or line.startswith(\"CLUSTAL\") or not line.strip():\n",
        "                continue\n",
        "\n",
        "            # Split the line into sequence name and sequence fragment\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            sequence_name, sequence_fragment = parts[0], parts[1]\n",
        "\n",
        "            # Append the fragment to the corresponding sequence\n",
        "            if sequence_name not in sequences:\n",
        "                sequences[sequence_name] = \"\"\n",
        "            sequences[sequence_name] += sequence_fragment\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# Function to check if specific residues are conserved, allowing for a set of acceptable residues\n",
        "def check_conserved_residues_with_gaps(reference_seq, generated_seq, residues_to_check):\n",
        "    ref_position = 0\n",
        "    conserved = True\n",
        "\n",
        "    for i in range(len(reference_seq)):  # Loop through the aligned sequence\n",
        "        if reference_seq[i] != '-':  # Only increment the reference position if it's not a gap\n",
        "            ref_position += 1\n",
        "\n",
        "        if ref_position in residues_to_check:  # Check only the positions of interest\n",
        "            ref_residue = reference_seq[i]\n",
        "            gen_residue = generated_seq[i]\n",
        "\n",
        "            allowed_residues = residues_to_check[ref_position]\n",
        "\n",
        "            # Check if the generated residue is within the allowed set\n",
        "            if gen_residue not in allowed_residues:\n",
        "                conserved = False\n",
        "\n",
        "    return conserved\n",
        "\n",
        "residues_to_check = {\n",
        "    141: 'D',  # Aspartate at position 141\n",
        "    142: ['I', 'V'],  # Isoleucine or Valine at position 142\n",
        "    143: 'E',  # Glutamate at position 143\n",
        "    210: 'N',  # Asparagine at position 210\n",
        "    214: 'F',  # Phenylalanine at position 214\n",
        "    215: 'D',  # Aspartate at position 215\n",
        "    274: ['S', 'T'], #Serine or Threonine at position 274\n",
        "    275: 'L',  # Leucine at position 275\n",
        "    278: ['A', 'V', 'L', 'I', 'M', 'F', 'W', 'P', 'Y'],  #Hydrophobic at position 278\n",
        "    287: 'K',  # Lysine at position 287\n",
        "    311: 'Y',  # Tyrosine at position 311\n",
        "    315: 'D',  # Aspartate at position 315\n",
        "    387: 'G',  # Glycine at position 387\n",
        "    390: 'V',  # Valine at position 390\n",
        "    405: 'D',  # Aspartate at position 405\n",
        "    408: ['S', 'A'], # Serine or Aspartate at position 408\n",
        "    409: 'L',  # Leucine at position 409\n",
        "    410: 'Y',  # Tyrosine at position 410\n",
        "    411: 'P',  # Proline at position 411\n",
        "    412: 'S',  # Serine at position 412\n",
        "    413: 'I',  # Isoleucine at position 413\n",
        "    414: 'I',  # Isoleucine at position 414\n",
        "    484: 'Q',  # Glutamine at position 484\n",
        "    488: 'K',  # Lysine at position 488\n",
        "    492: 'N',  # Asparagine at position 492\n",
        "    493: 'S',  # Serine at position 493\n",
        "    495: 'Y',  # Tyrosine at position 495\n",
        "    496: 'G',  # Glycine at position 496\n",
        "    518: 'G',  # Glycine at position 518\n",
        "    541: 'D',  # Aspartate at position 541\n",
        "    542: 'T',  # Threonine at position 542\n",
        "    543: 'D',  # Aspartate at position 543\n",
        "    581: 'E',  # Glutamate at position 581\n",
        "    592: 'K',  # Lysine at position 592\n",
        "    593: 'K',  # Lysine at position 593\n",
        "    595: 'Y',  # Tyrosine at position 595\n",
        "    596: 'A',  # Alanine at position 596\n",
        "    608: 'G',  # Glycine at position 608\n",
        "    610: 'E',  # Glutamate at position 610\n",
        "}\n",
        "\n",
        "# Example sequences\n",
        "sequence_2 = \"MILDVDYITEEGKPVIRLFKKENGKFKIEHDRTFRPYIYALLRDDSKIEEVKKITGERHGKIVRIVDVEKVEKKFLGKPITVWKLYLEHPQDVPTIREKVREHPAVVDIFEYDIPFAKRYLIDKGLIPMEGEEELKILAFDIETLYHEGEEFGKGPIIMISYADENEAKVITWKNIDLPYVEVVSSEREMIKRFLRIIREKDPDIIVTYNGDSFDFPYLAKRAEKLGIKLTIGRDGSEPKMQRIGDMTAVEVKGRIHFDLYHVITRTINLPTYTLEAVYEAIFGKPKEKVYADEIAKAWESGENLERVAKYSMEDAKATYELGKEFLPMEIQLSRLVGQPLWDVSRSSTGNLVEWFLLRKAYERNEVAPNKPSEEEYQRRLRESYTGGFVKEPEKGLWENIVYLDFRALYPSIIITHNVSPDTLNLEGCKNYDIAPQVGHKFCKDIPGFIPSLLGHLLEERQKIKTKMKETQDPIEKILLDYRQKAIKLLANSFYGYYGYAKARWYCKECAESVTAWGRKYIELVWKELEEKFGFKVLYIDTDGLYATIPGGESEEIKKKALEFVKYINSKLPGLLELEYEGFYKRGFFVTKKRYAVIDEEGKVITRGLEIVRRDWSEIAKETQARVLETILKHGDVEEAVRIVKEVIQKLANYEIPPEKLAIYEQITRPLHEYKAIGPHVAVAKKLAAKGVKIKPGMVIGYIVLRGDGPISNRAILAEEYDPKKHKYDAEYYIENQVLPAVLRILEGFGYRKEDLRYQKTRQVGLTSWLNIKKS\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fx-h1MtotMK",
        "outputId": "2d55a52a-0131-4296-e547-e3e39f4ba04a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1059\n"
          ]
        }
      ],
      "source": [
        "# Define more functions and run the filtering process\n",
        "\n",
        "def is_degenerate(sequence):\n",
        "    # Define the k-mer sizes and the maximum allowed repetitions\n",
        "    k_mer_rules = {\n",
        "        6: 2,  # 6-mer cannot repeat more than 2 times consecutively\n",
        "        5: 3,\n",
        "        4: 3,  # 4-mer cannot repeat more than 3 times consecutively\n",
        "        3: 6,  # 3-mer cannot repeat more than 6 times consecutively\n",
        "        2: 8,   # 2-mer cannot repeat more than 8 times consecutively\n",
        "        7: 2,\n",
        "        8: 2\n",
        "    }\n",
        "\n",
        "    for k, max_repeats in k_mer_rules.items():\n",
        "        # Check each k-mer in the sequence\n",
        "        for i in range(len(sequence) - k + 1):\n",
        "            k_mer = sequence[i:i + k]\n",
        "            repeat_count = sequence.count(k_mer * max_repeats)\n",
        "            if repeat_count > 0:\n",
        "                return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def contains_non_canonical_amino_acids(sequence):\n",
        "    # Canonical amino acids\n",
        "    canonical_amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "    # Check if any amino acid in the sequence is non-canonical\n",
        "    return any(aa not in canonical_amino_acids for aa in sequence)\n",
        "\n",
        "def uncompleted(sequence):\n",
        "    if (sequence[0] == 'M'):\n",
        "      return False\n",
        "    else:\n",
        "      return True\n",
        "\n",
        "\n",
        "# Full pipeline: check conserved residues, filter degenerate, and non-canonical sequences\n",
        "for seq in sequences[:]:  # Using a copy of the list for safe removal\n",
        "    if is_degenerate(seq) or contains_non_canonical_amino_acids(seq) or uncompleted(seq):\n",
        "        sequences.remove(seq)\n",
        "\n",
        "for seq in sequences[:]:\n",
        "    sequence_1 = seq\n",
        "    alignment_result = run_clustal_omega(sequence_1, sequence_2, \"aligned_sequences.aln\")\n",
        "\n",
        "    alignment_file = \"aligned_sequences.aln\"\n",
        "    aligned_sequences = parse_alignment_file(alignment_file)\n",
        "\n",
        "    reference_seq = aligned_sequences['Sequence2']\n",
        "    generated_seq = aligned_sequences['Sequence1']\n",
        "\n",
        "    residues_conserved = check_conserved_residues_with_gaps(reference_seq, generated_seq, residues_to_check)\n",
        "\n",
        "    # Corrected comparison and list removal\n",
        "    if residues_conserved == False:\n",
        "        sequences.remove(seq)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter sequences further based on sequence perplexity scores\n",
        "\n",
        "for seq in sequences[:]:\n",
        "    #perplexity filtering\n",
        "    tokenized_sequence = tokenizer.encode(f'1{seq}2')\n",
        "    input_tensor = torch.tensor([tokenized_sequence.ids]).to('cuda')\n",
        "    output = model(input_tensor)\n",
        "    logits = output.logits\n",
        "\n",
        "    # Convert logits to probabilities\n",
        "    probabilities = softmax(logits, dim=-1)\n",
        "\n",
        "    # Calculate cross-entropy\n",
        "    true_token_ids = input_tensor[:, 1:]  # Shifted true token IDs for labels\n",
        "    log_probs = torch.log(probabilities)\n",
        "\n",
        "    # Fix dimension mismatch by unsqueezing true_token_ids\n",
        "    true_token_ids = true_token_ids.unsqueeze(-1)  # Now true_token_ids shape is (batch_size, seq_length-1, 1)\n",
        "\n",
        "    # Gather log_probs based on the true_token_ids\n",
        "    selected_log_probs = torch.gather(log_probs[:, :-1, :], dim=-1, index=true_token_ids)\n",
        "\n",
        "    # Calculate cross-entropy and perplexity\n",
        "    cross_entropy = -torch.mean(selected_log_probs)\n",
        "    perplexity = torch.exp(cross_entropy)\n",
        "\n",
        "    if (perplexity.item() - 2.2079501152038574) > 0.7:\n",
        "      sequences.remove(seq)\n",
        "\n",
        "print(len(sequences))"
      ],
      "metadata": {
        "id": "CMi7smmPi6I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out sequences with 80% sequence identity to each other\n",
        "\n",
        "input_fasta = \"input_sequences.fasta\"\n",
        "with open(input_fasta, \"w\") as f:\n",
        "    for i, seq in enumerate(sequences):\n",
        "        f.write(f\">seq{i}\\n{seq}\\n\")\n",
        "\n",
        "!cd-hit -i input_sequences.fasta -o clustered_sequences.fasta -c 0.8 -n 5 -d 0\n",
        "\n",
        "def read_fasta(filename):\n",
        "    filtered = []\n",
        "    with open(filename, 'r') as f:\n",
        "        seq = \"\"\n",
        "        for line in f:\n",
        "            if line.startswith(\">\"):\n",
        "                if seq:\n",
        "                    filtered.append(seq)\n",
        "                    seq = \"\"\n",
        "            else:\n",
        "                seq += line.strip()\n",
        "        if seq:\n",
        "            filtered.append(seq)\n",
        "    return filtered\n",
        "\n",
        "sequences = read_fasta(\"clustered_sequences.fasta\")"
      ],
      "metadata": {
        "id": "vufgUTJ5m760"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load progen2-base for checking log likelihood\n",
        "\n",
        "!git clone https://github.com/salesforce/progen\n",
        "%cd progen/progen2\n",
        "\n",
        "# checkpoint\n",
        "model='progen2-base'\n",
        "!wget -P checkpoints/progen2-base https://storage.googleapis.com/sfr-progen-research/checkpoints/progen2-base.tar.gz\n",
        "!tar -xvf checkpoints/progen2-base/progen2-base.tar.gz -C checkpoints/progen2-base/\n",
        "\n",
        "!pip3 install --upgrade pip setuptools\n",
        "!pip3 install -r requirements.txt\n",
        "%cd ..\n",
        "%cd .."
      ],
      "metadata": {
        "id": "DcNzr77Tjjgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4owH6hSJafp"
      },
      "outputs": [],
      "source": [
        "# Load ESM model for checking log likelihood\n",
        "\n",
        "!pip install fair-esm\n",
        "\n",
        "import torch\n",
        "import esm\n",
        "from transformers import AutoModelForMaskedLM\n",
        "\n",
        "# Load ESM-2 model\n",
        "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "\n",
        "# Load the ESM-2 3B model from Hugging Face\n",
        "model_name = \"facebook/esm2_t36_3B_UR50D\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name, trust_remote_code=True)\n",
        "model.eval()  # disables dropout for deterministic results\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L-58TtXkBF2"
      },
      "outputs": [],
      "source": [
        "# Compute log likelihood of sequences with ESM\n",
        "\n",
        "def compute_pseudolikelihoods(sequences):\n",
        "    # Store pseudolikelihoods\n",
        "    pseudolikelihoods = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sequence in sequences:\n",
        "            # Prepare the sequence for the model\n",
        "            batch_labels, batch_strs, batch_tokens = batch_converter([(\"protein\", sequence)])\n",
        "            batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "            # Calculate log-likelihoods\n",
        "            logits = model(batch_tokens)[\"logits\"]\n",
        "            log_probs = logits.log_softmax(dim=-1)\n",
        "\n",
        "            # Extract pseudolikelihood for the sequence\n",
        "            sequence_length = batch_tokens.shape[1] - 2  # Exclude special tokens ([CLS], [EOS])\n",
        "            batch_tokens_no_special = batch_tokens[:, 1:-1]\n",
        "\n",
        "            # Masked positions' probabilities\n",
        "            masked_probs = log_probs[0, torch.arange(1, sequence_length + 1), batch_tokens_no_special[0]]\n",
        "            pseudolikelihood = (masked_probs.sum().item())/len(sequence)\n",
        "\n",
        "            pseudolikelihoods.append(pseudolikelihood)\n",
        "\n",
        "    return pseudolikelihoods\n",
        "\n",
        "likelihoodsESM = compute_pseudolikelihoods(sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATdMw9GvRi7d"
      },
      "outputs": [],
      "source": [
        "# Compute log likelihoods with ProGen2\n",
        "\n",
        "import json\n",
        "\n",
        "with open('sequences.json', 'w') as f:\n",
        "    json.dump(sequences, f)\n",
        "\n",
        "%cd progen/progen2\n",
        "!python3 likelihood.py --context-file sequences.json\n",
        "\n",
        "with open('likelihoods.json', 'r') as f:\n",
        "    likelihoodsProGen = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHkDfHAaDm94"
      },
      "outputs": [],
      "source": [
        "# Filter down to the top 100 sequences with the highest log likelihood scores\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "likelihoodsProGen = np.array(likelihoodsProGen)  # Replace with your Progen2 likelihoods\n",
        "likelihoodsESM = np.array(likelihoodsESM)    # Replace with your ESM-2 likelihoods\n",
        "\n",
        "average_likelihoods = (likelihoodsProGen + likelihoodsESM) / 2\n",
        "\n",
        "closest_indices = np.argsort(np.abs(average_likelihoods))[:100]\n",
        "\n",
        "sequences = [sequences[i] for i in closest_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1c4NNTCE8mt"
      },
      "outputs": [],
      "source": [
        "# Creates a FASTA file of the sequences to input to NetSolP\n",
        "\n",
        "def write_fasta(sequences, output_file, headers=None):\n",
        "\n",
        "    if headers is None:\n",
        "        headers = [f\"seq{i+1}\" for i in range(len(sequences))]\n",
        "    if len(sequences) != len(headers):\n",
        "        raise ValueError(\"The number of headers must match the number of sequences.\")\n",
        "\n",
        "    with open(output_file, \"w\") as fasta_file:\n",
        "        for header, sequence in zip(headers, sequences):\n",
        "            fasta_file.write(f\">{header}\\n\")\n",
        "            fasta_file.write(f\"{sequence}\\n\")\n",
        "    print(f\"FASTA file written to {output_file}\")\n",
        "\n",
        "output_path = \"protein_sequences.fasta\"\n",
        "\n",
        "write_fasta(sequences, output_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Arrange results from NetSolP and filter down to the top 20\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "sequences = pd.read_csv('NetSolP.csv')\n",
        "fasta = sequences['fasta'].to_list()\n",
        "solubility = sequences['predicted_solubility'].to_list()\n",
        "usability = sequences['predicted_usability'].to_list()\n",
        "\n",
        "combined = list(zip(fasta, solubility, usability))\n",
        "\n",
        "sorted_combined = sorted(combined, key=lambda x: x[2])\n",
        "\n",
        "sorted_fasta, sorted_solubility, sorted_usability = zip(*sorted_combined)\n",
        "\n",
        "sorted_fasta = list(sorted_fasta)\n",
        "sorted_solubility = list(sorted_solubility)\n",
        "sorted_usability = list(sorted_usability)\n",
        "\n",
        "selected_sequences = sorted_fasta[80:]"
      ],
      "metadata": {
        "id": "PSqFH8jjZ2Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sorted_solubility[80:100])\n",
        "print(sorted_usability[80:100])"
      ],
      "metadata": {
        "id": "3O2yHpv_GZsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create CSV file with the sequences to input into Boltz\n",
        "\n",
        "with open('batchX_selected.csv', mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write each string as a new row\n",
        "    for string in selected_sequences:\n",
        "        writer.writerow([string])\n",
        "\n",
        "print(\"CSV file created successfully.\")"
      ],
      "metadata": {
        "id": "clNBurWKd3xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a FASTA file for each sequence in a directory called \"sequences\"\n",
        "\n",
        "import os\n",
        "\n",
        "# Path to your CSV file\n",
        "csv_file_path = \"selected_sequences2.csv\"\n",
        "\n",
        "# Create the 'sequences' directory if it doesn't exist\n",
        "output_dir = \"sequences\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Read the CSV file and create FASTA files\n",
        "with open(csv_file_path, \"r\") as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "\n",
        "    # Process each row (single column: sequence)\n",
        "    for row_number, row in enumerate(csv_reader, start=1):\n",
        "        if not row:\n",
        "            continue  # Skip empty rows\n",
        "\n",
        "        sequence = row[0]\n",
        "        fasta_file_path = os.path.join(output_dir, f\"sequence{row_number}.fasta\")\n",
        "\n",
        "        # Write to the FASTA file\n",
        "        with open(fasta_file_path, \"w\") as fasta_file:\n",
        "            fasta_file.write(f\">A|protein\\n{sequence}\")\n",
        "\n",
        "print(f\"FASTA files have been created in the '{output_dir}' directory.\")"
      ],
      "metadata": {
        "id": "ndgJ-b39cX-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting predictions file\n",
        "\n",
        "import zipfile\n",
        "\n",
        "zip_file_path = 'predictions.zip'\n",
        "extract_dir = 'predictions'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(f\"Files extracted to {extract_dir}\")"
      ],
      "metadata": {
        "id": "C8neu1IOGdKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Arrange sequences from lowest to highest confidence scores\n",
        "\n",
        "parent_dir = 'predictions/boltz_results_sequences/predictions'\n",
        "\n",
        "# List to store the confidence scores\n",
        "confidence_scores = []\n",
        "directory_names = []\n",
        "\n",
        "# Walk through the directories in the parent directory\n",
        "for dir_name in os.listdir(parent_dir):\n",
        "    dir_path = os.path.join(parent_dir, dir_name)\n",
        "\n",
        "    # Check if the directory name starts with 'sequence'\n",
        "    if os.path.isdir(dir_path) and dir_name.startswith('sequence'):\n",
        "        # Look for the JSON file in the directory\n",
        "        for file_name in os.listdir(dir_path):\n",
        "            if file_name.endswith('.json'):\n",
        "                json_file_path = os.path.join(dir_path, file_name)\n",
        "\n",
        "                # Read the JSON file and extract the confidence score\n",
        "                with open(json_file_path, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                    # Append the confidence score and directory name\n",
        "                    confidence_scores.append(data.get('confidence_score'))\n",
        "                    directory_names.append(dir_name)\n",
        "\n",
        "# Combine the lists and sort by confidence scores\n",
        "sorted_pairs = sorted(zip(confidence_scores, directory_names))\n",
        "\n",
        "# Unzip the sorted pairs back into separate lists\n",
        "sorted_confidence_scores, sorted_directory_names = zip(*sorted_pairs)\n",
        "\n",
        "# Convert them back to lists (if needed)\n",
        "sorted_confidence_scores = list(sorted_confidence_scores)\n",
        "sorted_directory_names = list(sorted_directory_names)\n",
        "\n",
        "# Print the sorted confidence scores and corresponding directory names\n",
        "print(\"Sorted Confidence Scores:\", sorted_confidence_scores)\n",
        "print(\"Sorted Directory Names:\", sorted_directory_names)"
      ],
      "metadata": {
        "id": "0DO2_jQwGwMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3: Fidelity Scoring**"
      ],
      "metadata": {
        "id": "TvSVNatenlav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using BioEmu to obtain a simulation of the DNA polymerase (input sequence and replace X with sequence number)\n",
        "\n",
        "from bioemu.sample import main as sample\n",
        "sample(sequence='', num_samples=1000, output_dir='./sequenceX')"
      ],
      "metadata": {
        "id": "-TQ1e8JjnsG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter positions of aspartate residues in designed polymerase (D405 and D541 in WT Pfu)\n",
        "\n",
        "!apt-get install -y clustalo\n",
        "\n",
        "def run_clustal_omega(seq1, seq2, output_file):\n",
        "    with open(\"input_sequences.fasta\", \"w\") as f:\n",
        "        f.write(f\">Sequence1\\n{seq1}\\n\")\n",
        "        f.write(f\">Sequence2\\n{seq2}\\n\")\n",
        "\n",
        "    clustal_cmd = [\n",
        "        \"clustalo\",\n",
        "        \"-i\", \"input_sequences.fasta\",\n",
        "        \"-o\", output_file,\n",
        "        \"--force\",\n",
        "        \"--outfmt\", \"clu\"\n",
        "    ]\n",
        "\n",
        "    subprocess.run(clustal_cmd)\n",
        "\n",
        "    with open(output_file, \"r\") as aligned_file:\n",
        "        alignment = aligned_file.read()\n",
        "    return alignment\n",
        "\n",
        "# Function to parse the Clustal Omega alignment file\n",
        "def parse_alignment_file(alignment_file):\n",
        "    sequences = {}\n",
        "    with open(alignment_file, 'r') as file:\n",
        "        current_sequence = \"\"\n",
        "        for line in file:\n",
        "            # Skip headers or other non-sequence lines\n",
        "            if line.startswith(\" \") or line.startswith(\"CLUSTAL\") or not line.strip():\n",
        "                continue\n",
        "\n",
        "            # Split the line into sequence name and sequence fragment\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            sequence_name, sequence_fragment = parts[0], parts[1]\n",
        "\n",
        "            # Append the fragment to the corresponding sequence\n",
        "            if sequence_name not in sequences:\n",
        "                sequences[sequence_name] = \"\"\n",
        "            sequences[sequence_name] += sequence_fragment\n",
        "\n",
        "    return sequences\n",
        "\n",
        "def check_for_aspartate(reference_seq, generated_seq, residues_to_check):\n",
        "    ref_position = 0\n",
        "    gaps = 0\n",
        "    conserved = True\n",
        "\n",
        "    for i in range(len(reference_seq)):  # Loop through the aligned sequence\n",
        "        if reference_seq[i] != '-':  # Only increment the reference position if it's not a gap\n",
        "            ref_position += 1\n",
        "\n",
        "        if generated_seq[i] == '-':\n",
        "            gaps += 1\n",
        "\n",
        "        if ref_position in residues_to_check:  # Check only the positions of interest\n",
        "            return i-gaps+1\n",
        "\n",
        "sequence_2 = \"MILDVDYITEEGKPVIRLFKKENGKFKIEHDRTFRPYIYALLRDDSKIEEVKKITGERHGKIVRIVDVEKVEKKFLGKPITVWKLYLEHPQDVPTIREKVREHPAVVDIFEYDIPFAKRYLIDKGLIPMEGEEELKILAFDIETLYHEGEEFGKGPIIMISYADENEAKVITWKNIDLPYVEVVSSEREMIKRFLRIIREKDPDIIVTYNGDSFDFPYLAKRAEKLGIKLTIGRDGSEPKMQRIGDMTAVEVKGRIHFDLYHVITRTINLPTYTLEAVYEAIFGKPKEKVYADEIAKAWESGENLERVAKYSMEDAKATYELGKEFLPMEIQLSRLVGQPLWDVSRSSTGNLVEWFLLRKAYERNEVAPNKPSEEEYQRRLRESYTGGFVKEPEKGLWENIVYLDFRALYPSIIITHNVSPDTLNLEGCKNYDIAPQVGHKFCKDIPGFIPSLLGHLLEERQKIKTKMKETQDPIEKILLDYRQKAIKLLANSFYGYYGYAKARWYCKECAESVTAWGRKYIELVWKELEEKFGFKVLYIDTDGLYATIPGGESEEIKKKALEFVKYINSKLPGLLELEYEGFYKRGFFVTKKRYAVIDEEGKVITRGLEIVRRDWSEIAKETQARVLETILKHGDVEEAVRIVKEVIQKLANYEIPPEKLAIYEQITRPLHEYKAIGPHVAVAKKLAAKGVKIKPGMVIGYIVLRGDGPISNRAILAEEYDPKKHKYDAEYYIENQVLPAVLRILEGFGYRKEDLRYQKTRQVGLTSWLNIKKS\""
      ],
      "metadata": {
        "id": "TM9TuexQog_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"MILDVDYITEEGKPVIRLFKKENGKFKIEHDRTFRPYIYALLRDDSKIEEVKKITGERHGKIVRIVDVEKVEKKFLGKPITVWKLYLEHPQDVPTIREKVREHPAVVDIFEYDIPFAKRYLIDKGLIPMEGEEELKILAFDIETLYHEGEEFGKGPIIMISYADENEAKVITWKNIDLPYVEVVSSEREMIKRFLRIIREKDPDIIVTYNGDSFDFPYLAKRAEKLGIKLTIGRDGSEPKMQRIGDMTAVEVKGRIHFDLYHVITRTINLPTYTLEAVYEAIFGKPKEKVYADEIAKAWESGENLERVAKYSMEDAKATYELGKEFLPMEIQLSRLVGQPLWDVSRSSTGQLVEWFLLRKAYERNELVPNKPSEKEYAQRKGSYAGGYVKEPLRGLHENIALFDFRSLYPSIIVTHNIGPDTLNCKCCKDEEYEVPGFQFMFCKKRKGFIPAAIEEIIERRVRVKELMKGASDPLEKQLLDARQFALKIIANSFYGYLGWFRARWYKQECAASVTAFGRYYIGQVIKAAEEEGFKVLYGDTDSLFVTVGKNSPEEAIKFLERINRELPGVMELELEGFYPRGIFVSKKRYAVISDDGKITVKGLEAVRRDWSVIAKEVQTKVLEIILREGDPKKAAELVRKVIKDLREGRVPIDKLIIYTQLTKDPNKYEATAPHVRAAKKLMEMGYKVGKGDKIGYVIVKGSGKVSERAYPYEKVSSKDYDVDYYIKNQIIPAVMRVLEAFGVKEEELIAGGKQIDLFRFG\"\n",
        "\n",
        "alignment_result = run_clustal_omega(sequence, sequence_2, \"aligned_sequences.aln\")\n",
        "\n",
        "alignment_file = \"aligned_sequences.aln\"\n",
        "aligned_sequences = parse_alignment_file(alignment_file)\n",
        "\n",
        "reference_seq = aligned_sequences['Sequence2']\n",
        "generated_seq = aligned_sequences['Sequence1']\n",
        "\n",
        "aspartate_residue1 = check_for_aspartate(reference_seq, generated_seq, {405: 'D'})\n",
        "aspartate_residue2 = check_for_aspartate(reference_seq, generated_seq, {541: 'D'})"
      ],
      "metadata": {
        "id": "Xs1H5tVyRuHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aspartate_residue1 = 2\n",
        "aspartate_residue2 = 3"
      ],
      "metadata": {
        "id": "QkgZKm-WXcz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install MDAnalysis\n",
        "\n",
        "import MDAnalysis as mda\n",
        "from MDAnalysis.analysis import rms, distances\n",
        "from MDAnalysis.analysis.rms import RMSD, RMSF\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "u = mda.Universe(\"topology1.pdb\", \"samples1.xtc\")\n",
        "\n",
        "alignment = RMSD(u, u, select=\"backbone\", ref_frame=0)\n",
        "alignment.run()\n",
        "rmsd_times = alignment.rmsd[:,1]\n",
        "rmsd_values = alignment.rmsd[:,2]\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(rmsd_times, rmsd_values)\n",
        "plt.xlabel(\"Time (ps)\")\n",
        "plt.ylabel(\"Backbone RMSD (Å)\")\n",
        "plt.title(\"Backbone RMSD vs. Time\")\n",
        "plt.show()\n",
        "\n",
        "prot = u.select_atoms(\"protein and backbone\")\n",
        "rmsf_calc = RMSF(prot).run()\n",
        "\n",
        "ix_to_rmsf = dict(zip(prot.atoms, rmsf_calc.rmsf))\n",
        "\n",
        "rmsf_per_residue = []\n",
        "resnums = []\n",
        "\n",
        "for res in prot.residues:\n",
        "    res_rmsfs = [ix_to_rmsf[atom] for atom in res.atoms if atom in ix_to_rmsf]\n",
        "    if res_rmsfs:\n",
        "        rmsf_per_residue.append(np.mean(res_rmsfs))\n",
        "        resnums.append(res.resid)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(resnums, rmsf_per_residue)\n",
        "plt.xlabel(\"Residue Number\")\n",
        "plt.ylabel(\"Backbone RMSF (Å)\")\n",
        "plt.title(\"Backbone RMSF per Residue\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "sel1 = u.select_atoms(f\"resid {aspartate_residue1} and name CA\")\n",
        "sel2 = u.select_atoms(f\"resid {aspartate_residue2} and name CA\")\n",
        "distances_over_time = []\n",
        "times = []\n",
        "\n",
        "for ts in u.trajectory:\n",
        "    pos1 = sel1.positions\n",
        "    pos2 = sel2.positions\n",
        "    dist = np.linalg.norm(pos1 - pos2)\n",
        "    distances_over_time.append(dist)\n",
        "    times.append(ts.time)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(times, distances_over_time)\n",
        "plt.xlabel(\"Time (ps)\")\n",
        "plt.ylabel(\"Distance (Å)\")\n",
        "plt.title(\"Catalytic CA–CA Distance vs. Time\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Average backbone RMSD: {np.mean(rmsd_values):.2f} Å ± {np.std(rmsd_values):.2f} Å\")\n",
        "catalytic_residues = [aspartate_residue1, aspartate_residue2]\n",
        "active_site_rmsfs = [r for r, n in zip(rmsf_per_residue, resnums) if n in catalytic_residues]\n",
        "print(f\"Max RMSF in active-site residues: {max(active_site_rmsfs):.2f} Å\")\n",
        "print(f\"Average catalytic distance: {np.mean(distances_over_time):.2f} Å\")"
      ],
      "metadata": {
        "id": "YV-2QoAFoMHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import MDAnalysis as mda\n",
        "from MDAnalysis.analysis import align, pca\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "selection = \"protein and name CA\"\n",
        "\n",
        "atoms = u.select_atoms(selection)\n",
        "\n",
        "aligner = align.AlignTraj(u, u, select=selection, in_memory=True)\n",
        "aligner.run()\n",
        "\n",
        "pca_analysis = pca.PCA(u, select=selection, align=False)\n",
        "pca_analysis.run()\n",
        "\n",
        "projections = pca_analysis.transform(atoms, n_components=2)\n",
        "\n",
        "print(\"Variance explained by PC1 and PC2:\")\n",
        "print(pca_analysis.results.variance[:2])"
      ],
      "metadata": {
        "id": "vPfxYUdGBFbN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
